# -*- coding: utf-8 -*-
"""UGRP_CH21B058.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEuDSbgghd1UQG-fmB4cWRiKnBeWLOgY

Installing Libraries Y Finance and Google News
"""

!pip install GoogleNews
!pip install --upgrade --no-cache-dir googlesearch-python
!pip install yfinance

from GoogleNews import GoogleNews
import pandas as pd
import time

"""Selecting the Relevant SET50 stocks"""

from GoogleNews import GoogleNews
import pandas as pd
import time

set50_companies = [
     "ADVANC", "AOT", "AWC", "BANPU", "BBL", "BDMS", "BEM", "BGRIM", "BH", "BJC",
    "CBG", "CENTEL", "COM7", "CPALL", "CPF", "CPN", "DELTA", "DOHOME", "EGCO","EA", "GLOBAL",
    "GPSC", "HMPRO", "INTUCH", "IRPC", "IVL", "JMART", "KBANK", "KCE", "KTB",
    "KTC", "MEGA", "MINT", "MTC", "OR", "OSP", "PTT", "PTTEP", "PTTGC", "RATCH",
    "SAWAD", "SCB", "SCC", "SCGP", "SGP", "TISCO", "TOP", "TRUE", "TTB", "TU"
]

googlenews = GoogleNews(lang="en",region='THAILAND')

all_articles = []

for company in set50_companies:
    print(f"Fetching news for: {company}")

    query = f"{company} Thailand stock"

    try:
        googlenews.search(query)
        results = googlenews.results()
        print(f"Articles found: {len(results)}")
    except Exception as e:
        print(f"Failed for {company}: {e}")
        continue

    for article in results:
        all_articles.append({
            "company": company,
            "date_raw": article.get("date"),
            "title": article.get("title"),
            "description": article.get("desc"),
            "source": article.get("media"),
            "url": article.get("link")
        })

    googlenews.clear()
    time.sleep(2)

news_df = pd.DataFrame(all_articles)

print("Total articles collected:", len(news_df))
news_df.head()

news_df

"""Date Time Mismatch Clearance"""

from datetime import datetime

scrape_time = datetime.now()

import re
from datetime import timedelta
import pandas as pd

def parse_raw_date(raw_date, reference_time):
    if not isinstance(raw_date, str):
        return None

    raw_date = raw_date.lower().strip()

    match = re.match(r"(\d+)\s+(day|days|week|weeks|hour|hours)\s+ago", raw_date)

    if match:
        value = int(match.group(1))
        unit = match.group(2)

        if "week" in unit:
            return reference_time - timedelta(weeks=value)
        elif "day" in unit:
            return reference_time - timedelta(days=value)
        elif "hour" in unit:
            return reference_time - timedelta(hours=value)
    try:
        return pd.to_datetime(raw_date)
    except:
        return None

news_df["date_parsed"] = news_df["date_raw"].apply(
    lambda x: parse_raw_date(x, scrape_time)
)

news_df = news_df.dropna(subset=["date_parsed"])
news_df = news_df.sort_values("date_parsed").reset_index(drop=True)
news_df["date_daily"] = news_df["date_parsed"].dt.floor("D")

news_df

"""Installing Transformers and Hugging Face for FinBERT LLM call for sentiment analysis"""

!pip install transformers torch --quiet

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax

tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
model.eval()

news_df["text"] = (
    news_df["title"].fillna("") + ". " +
    news_df["description"].fillna("")
).str.strip()

"""Sentiment Scores for outputs"""

label_map = {
    0: "negative",
    1: "neutral",
    2: "positive"
}

score_map = {
    "negative": -1.0,
    "neutral": 0.0,
    "positive": 1.0
}

def finbert_sentiment(text):
    if not text or text.strip() == "":
        return 0.0  # no information

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512
    )

    with torch.no_grad():
        outputs = model(**inputs)
        probs = softmax(outputs.logits, dim=1)[0]

    label_id = torch.argmax(probs).item()
    label = label_map[label_id]

    return score_map[label]

news_df["sentiment_score"] = news_df["text"].apply(finbert_sentiment)

news_df

"""Selecting only the companies where news articles were available"""

daily_sentiment = (
    news_df
    .groupby(["company", "date_daily"])
    .agg(
        avg_sentiment=("sentiment_score", "mean"),
        article_count=("sentiment_score", "count")
    )
    .reset_index()
)

daily_sentiment

unique_names = daily_sentiment['company'].unique()
print(len(unique_names))

import yfinance as yf

tickers_with_sentiment = daily_sentiment["company"].unique()

"""CLOSE OPEN VOLUME for the relevant tickers"""

import yfinance as yf
import time

tickers = [f"{ticker}.BK" for ticker in tickers_with_sentiment]

def download_in_batches(tickers, batch_size=10):
    combined_df = []
    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i + batch_size]
        try:
            print(f"Downloading batch: {batch}")
            data = yf.download(
                tickers=batch,
                period="2y",
                interval="1d",
                group_by='ticker',
                auto_adjust=True,
                threads=False
            )

            for ticker in batch:
                if ticker in data.columns.get_level_values(0):
                    df_prices = data[ticker].dropna().copy()
                    df_prices['Ticker'] = ticker.replace('.BK', '')
                    df_prices.reset_index(inplace=True)
                    combined_df.append(df_prices)

            time.sleep(5)
        except Exception as e:
            print(f"Error downloading {batch}: {e}")
            time.sleep(10)

    return pd.concat(combined_df, ignore_index=True)

stock_df = download_in_batches(tickers)

stock_df

daily_sentiment

daily_sentiment = daily_sentiment.rename(columns={"company": "ticker"})
daily_sentiment["date_daily"] = pd.to_datetime(daily_sentiment["date_daily"])

stock_df = stock_df.rename(
    columns={
        "Date": "date_daily",
        "Ticker": "ticker",
        "Close": "close_price"
    }
)
stock_df["date_daily"] = pd.to_datetime(stock_df["date_daily"])

daily_sentiment = daily_sentiment.sort_values(["ticker", "date_daily"])
stock_df = stock_df.sort_values(["ticker", "date_daily"])

daily_sentiment

stock_df

stock_df = stock_df.dropna(subset=["ticker", "date_daily"])
daily_sentiment = daily_sentiment.dropna(subset=["ticker", "date_daily"])

daily_sentiment

stock_df = stock_df.sort_values(["ticker", "date_daily"]).reset_index(drop=True)
daily_sentiment = daily_sentiment.sort_values(["ticker", "date_daily"]).reset_index(drop=True)

daily_sentiment

merged_df = pd.merge(
    stock_df,
    daily_sentiment,
    on=["ticker", "date_daily"],
    how="left"
)

"""Merging Dataframes"""

merged_df

merged_df["avg_sentiment"] = merged_df["avg_sentiment"].fillna(0)
merged_df["article_count"] = merged_df["article_count"].fillna(0)

"""Converting NAN sentiment days to neutral sentiment days"""

merged_df

merged_df["return"] = (
    merged_df
    .groupby("ticker")["close_price"]
    .pct_change()
)

merged_df = merged_df.dropna(subset=["return"])

merged_df

tickers = merged_df["ticker"].unique()

def train_test_split_ts(df, split_ratio=0.8):
    split_idx = int(len(df) * split_ratio)
    train = df.iloc[:split_idx]
    test = df.iloc[split_idx:]
    return train, test

len(tickers)

"""Feeding Different Tickers

"""

ticker = tickers[10]

df = merged_df[merged_df["ticker"] == ticker].copy()
df = df.sort_values("date_daily").reset_index(drop=True)

split = int(len(df) * 0.8)

train = df.iloc[:split]
test  = df.iloc[split:]

y_test = test["return"].values

"""PRICE ONLY ARIMA"""

from statsmodels.tsa.arima.model import ARIMA

arima_po = ARIMA(train["return"], order=(1,0,1))
arima_po_fit = arima_po.fit()

arima_po_preds = arima_po_fit.forecast(steps=len(test))

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

def evaluate(y_true, y_pred):
    return {
        "MSE": mean_squared_error(y_true, y_pred),
        "MAE": mean_absolute_error(y_true, y_pred),
        "Directional_Accuracy": np.mean(np.sign(y_true) == np.sign(y_pred))
    }

arima_po_metrics = evaluate(y_test, arima_po_preds)

"""PRICE+ EXOGENEOUS SIGNAL ARIMA"""

arima_sent = ARIMA(
    train["return"],
    exog=train[["avg_sentiment"]],
    order=(1,0,1)
)

arima_sent_fit = arima_sent.fit()

arima_sent_preds = arima_sent_fit.forecast(
    steps=len(test),
    exog=test[["avg_sentiment"]]
)

arima_sent_metrics = evaluate(y_test, arima_sent_preds)

"""BILSTM WITH NO SENTIMENT

SEQUENCE MAKER
"""

import numpy as np

def create_sequences(X, y, window=10):
    Xs, ys = [], []
    for i in range(len(X) - window):
        Xs.append(X[i:i+window])
        ys.append(y[i+window])
    return np.array(Xs), np.array(ys)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional

def build_bilstm(input_shape):
    model = Sequential([
        Bidirectional(LSTM(32), input_shape=input_shape),
        Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    return model

X_train = train["return"].values.reshape(-1,1)
y_train = train["return"].values

X_test = test["return"].values.reshape(-1,1)
y_test = test["return"].values

Xtr_seq, ytr_seq = create_sequences(X_train, y_train)
Xte_seq, yte_seq = create_sequences(X_test, y_test)

bilstm_po = build_bilstm((Xtr_seq.shape[1], Xtr_seq.shape[2]))
bilstm_po.fit(Xtr_seq, ytr_seq, epochs=10, batch_size=16, verbose=0)

bilstm_po_preds = bilstm_po.predict(Xte_seq).flatten()
bilstm_po_metrics = evaluate(yte_seq, bilstm_po_preds)

"""BILSTM With Senitment"""

X_train = train[["return", "avg_sentiment"]].values
y_train = train["return"].values

X_test = test[["return", "avg_sentiment"]].values
y_test = test["return"].values

Xtr_seq, ytr_seq = create_sequences(X_train, y_train)
Xte_seq, yte_seq = create_sequences(X_test, y_test)

bilstm_sent = build_bilstm((Xtr_seq.shape[1], Xtr_seq.shape[2]))
bilstm_sent.fit(Xtr_seq, ytr_seq, epochs=10, batch_size=16, verbose=0)

bilstm_sent_preds = bilstm_sent.predict(Xte_seq).flatten()
bilstm_sent_metrics = evaluate(yte_seq, bilstm_sent_preds)

"""Comparison of different models with and without sentiments"""

comparison = {
    "ARIMA_price": arima_po_metrics,
    "ARIMA_sentiment": arima_sent_metrics,
    "BiLSTM_price": bilstm_po_metrics,
    "BiLSTM_sentiment": bilstm_sent_metrics
}

comparison

import pandas as pd

comparison_df = (
    pd.DataFrame(comparison)
      .T
      .reset_index()
      .rename(columns={"index": "Model"})
)

comparison_df

sentiment_effect = {
    "ARIMA_MSE_change": (
        comparison["ARIMA_sentiment"]["MSE"] -
        comparison["ARIMA_price"]["MSE"]
    ),
    "BiLSTM_MSE_change": (
        comparison["BiLSTM_sentiment"]["MSE"] -
        comparison["BiLSTM_price"]["MSE"]
    ),
    "BiLSTM_DA_change": (
        comparison["BiLSTM_sentiment"]["Directional_Accuracy"] -
        comparison["BiLSTM_price"]["Directional_Accuracy"]
    )
}

sentiment_effect

comparison_df.set_index("Model")["MSE"].plot(
    kind="bar",
    title=f"Model Comparison for {ticker}",
    figsize=(8,4)
)